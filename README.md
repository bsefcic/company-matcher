# Companyâ€‘MatcherÂ APIÂ ğŸš€

ğŸ‘‹ **Welcome!** This public repository contains the solution I built for the **Veridion fullâ€‘stack coding challenge** (JuneÂ 2025).  It showcases a complete pipeline that

1. **Crawls** a list of \~1â€Š000 company websites,
2. **Extracts** phone numbers & socialâ€‘media profiles,
3. **Normalises & stores** them in PostgreSQL,
4. **Indexes** the data in ElasticsearchÂ 8, and
5. Serves a \`/match\` endpoint that returns the *single best* company profile for any combination of
   - **name**
   - **website**
   - **phone number**
   - **social mediaÂ URL**.

> **Matchâ€‘rate on Veridionâ€™s test file:** **100Â %** (32Â /Â 32 rows) ğŸ‰

---

## ğŸ—ºÂ Architecture

```
  CSVÂ (websites)              CSVÂ (companyÂ names)
        â”‚                             â”‚
        â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  website_queue   â”‚â—€â”€â”€â”€â”€â”€â”€ â”‚  seed:companies CLI â”‚
â”‚  (Postgres)      â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  status: PENDING â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1ï¸âƒ£ cron picks 30 URLs/min
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   2ï¸âƒ£ Playwright cluster
â”‚  ScraperÂ Runner      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Â + extract.utils      â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â–¼
                          3ï¸âƒ£ structured payload (phones/socials)
                                      â”‚
                                      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  companies â€¢ phones â€¢ socials (Postgres)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ 4ï¸âƒ£ bulk sync
                           â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  companies_v1 index (ES8) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ 5ï¸âƒ£ REST query
                         â–¼
            POSTÂ /match  âœ  best company + score
```

- **NestJSÂ v10 + TypeORMÂ v0.3 (PostgreSQL)** Â â€“ domain entities & CLI tasks.
- **PlaywrightÂ + playwrightâ€‘cluster** Â â€“ parallel headless scraping (4Â contexts).
- **libphonenumberâ€‘js** Â â€“ E.164 normalisation.
- **ElasticsearchÂ 8** Â â€“ fuzzyâ€name search + exact keyword filters.
- **Jest** Â â€“ extractor unit tests.

---

## ğŸƒÂ Quick start (DockerÂ Compose)

```bash
# 1. clone & install
pnpm i               # or npm i / yarn install

# 2. spin up DB + ES
docker compose up -d postgres elasticsearch

# 3. import CSVs âœ Postgres âœ Elasticsearch
npm run seed:websites  data/sample-websites.csv
npm run seed:companies data/sample-websites-company-names.csv
npm run sync:es

# 4. launch API & crawler
npm run start:dev          # starts Nest app (portÂ 3000) + cron crawler

# 5. test a match
curl -s -X POST http://localhost:3000/match \
  -H 'Content-Type: application/json' \
  -d '{"name":"DoLee Rentals","phone":"+12294369620"}' | jq

# 6. evaluate full matchâ€‘rate
npm run eval:match         # parses API-input-sample.csv âœ 100Â % âœ…
```

---

## ğŸ§©Â Key scripts

| Command                | Purpose                                                      |
| ---------------------- | ------------------------------------------------------------ |
| `seed:websites  <csv>` | Fill **website\_queue** with PENDING rows                    |
| `seed:companies <csv>` | Upsert companies & merge phones/socials from scraper payload |
| `sync:es`              | (Re)build *companies\_v1* index & bulkâ€‘load data             |
| `eval:match`           | Print matchâ€‘rate on Veridionâ€™s *API-input-sample.csv*        |
| `crawler` (script)     | Run only the Playwright worker (no HTTP)                     |

---

## ğŸ“Â Challenge brief (highâ€‘level)

> **Task.** Extract phone numbers, social media links and addresses from a list of \~1k websites, merge with a second CSV of company names, and expose an API that returns the best company profile given any subset of name/website/phone/facebook.
>
> **Scoring.** *Matchâ€‘rate* =Â #API rows correctly matched.  Bonus for *accuracy metric* & scalabilityÂ (â‰¤Â 10Â minutes for full crawl).

This repository delivers:

- **Scalable crawler** â€“ 4 headless contexts; 1Â k sites crawl in â‰ˆÂ 8Â min locally.
- **100Â % matchâ€‘rate** on the provided inputs (see `eval:match`).
- **Postgres + Elasticsearch** â€“ same stack Veridion mentions in the brief.
- **Docker compose** so reviewers can run everything in two commands.

---

## ğŸ“œÂ Licence

MIT â€“ share, fork, improve.  Credits appreciated.

---

> Built with â˜•, a stubborn regex obsession, and lots of retries.  Enjoy!
